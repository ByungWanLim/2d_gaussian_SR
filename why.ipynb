{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered models: dict_keys([])\n",
      "File not found: models/weights/edsr_baseline_x2-1bc95232.pt\n",
      "torch.Size([1, 64, 48, 48]) torch.Size([1, 100, 48, 48])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pook0\\AppData\\Local\\Temp\\ipykernel_4288\\3632505019.py:123: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(args.pretrained_path)\n"
     ]
    }
   ],
   "source": [
    "# modified from: https://github.com/thstkdgus35/EDSR-PyTorch\n",
    "import math\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import register\n",
    "\n",
    "\n",
    "def default_conv(in_channels, out_channels, kernel_size, bias=True):\n",
    "    return nn.Conv2d(\n",
    "        in_channels, out_channels, kernel_size,\n",
    "        padding=(kernel_size // 2), bias=bias)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, conv, n_feats, kernel_size,\n",
    "            bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n",
    "\n",
    "        super(ResBlock, self).__init__()\n",
    "        m = []\n",
    "        for i in range(2):\n",
    "            m.append(conv(n_feats, n_feats, kernel_size, bias=bias))\n",
    "            if bn:\n",
    "                m.append(nn.BatchNorm2d(n_feats))\n",
    "            if i == 0:\n",
    "                m.append(act)\n",
    "\n",
    "        self.body = nn.Sequential(*m)\n",
    "        self.res_scale = res_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x).mul(self.res_scale)\n",
    "        res += x\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "class Upsampler(nn.Sequential):\n",
    "    def __init__(self, conv, scale, n_feats, bn=False, act=False, bias=True):\n",
    "\n",
    "        m = []\n",
    "        if (scale & (scale - 1)) == 0:\n",
    "            for _ in range(int(math.log(scale, 2))):\n",
    "                m.append(conv(n_feats, 4 * n_feats, 3, bias))\n",
    "                m.append(nn.PixelShuffle(2))\n",
    "                if bn:\n",
    "                    m.append(nn.BatchNorm2d(n_feats))\n",
    "                if act == 'relu':\n",
    "                    m.append(nn.ReLU(True))\n",
    "                elif act == 'prelu':\n",
    "                    m.append(nn.PReLU(n_feats))\n",
    "\n",
    "        elif scale == 3:\n",
    "            m.append(conv(n_feats, 9 * n_feats, 3, bias))\n",
    "            m.append(nn.PixelShuffle(3))\n",
    "            if bn:\n",
    "                m.append(nn.BatchNorm2d(n_feats))\n",
    "            if act == 'relu':\n",
    "                m.append(nn.ReLU(True))\n",
    "            elif act == 'prelu':\n",
    "                m.append(nn.PReLU(n_feats))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        super(Upsampler, self).__init__(*m)\n",
    "\n",
    "\n",
    "class MeanShift(nn.Conv2d):\n",
    "    def __init__(\n",
    "            self, rgb_range,\n",
    "            rgb_mean=(0.4488, 0.4371, 0.4040), rgb_std=(1.0, 1.0, 1.0), sign=-1):\n",
    "        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n",
    "        std = torch.Tensor(rgb_std)\n",
    "        self.weight.data = torch.eye(3).view(3, 3, 1, 1) / std.view(3, 1, 1, 1)\n",
    "        self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean) / std\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "@register('edsr-baseline')\n",
    "class EDSR(nn.Module):\n",
    "    def __init__(self, args, conv=default_conv):\n",
    "        super(EDSR, self).__init__()\n",
    "        self.args = args\n",
    "        n_resblocks = args.n_resblocks\n",
    "        n_feats = args.n_feats\n",
    "        kernel_size = 3\n",
    "        scale = args.scale[0]\n",
    "        act = nn.ReLU(True)\n",
    "\n",
    "        # define head module\n",
    "        m_head = [conv(args.n_colors, n_feats, kernel_size)]\n",
    "\n",
    "        # define body module\n",
    "        m_body = [\n",
    "            ResBlock(\n",
    "                conv, n_feats, kernel_size, act=act, res_scale=args.res_scale\n",
    "            ) for _ in range(n_resblocks)\n",
    "        ]\n",
    "        m_body.append(conv(n_feats, n_feats, kernel_size))\n",
    "\n",
    "        self.head = nn.Sequential(*m_head)\n",
    "        self.body = nn.Sequential(*m_body)\n",
    "\n",
    "        if args.no_upsampling:\n",
    "            self.out_dim = n_feats\n",
    "        else:\n",
    "            self.out_dim = args.n_colors\n",
    "            # define tail module\n",
    "            m_tail = [\n",
    "                Upsampler(conv, scale, n_feats, act=False),\n",
    "                conv(n_feats, args.n_colors, kernel_size)\n",
    "            ]\n",
    "            self.tail = nn.Sequential(*m_tail)\n",
    "\n",
    "        self.sub_mean = MeanShift(args.rgb_range)\n",
    "        self.add_mean = MeanShift(args.rgb_range, sign=1)\n",
    "\n",
    "        if args.pretrained_path:\n",
    "            try:\n",
    "                pretrained_dict = torch.load(args.pretrained_path)\n",
    "                self.load_state_dict(pretrained_dict)\n",
    "                print(\"Pretrained model loaded successfully.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {args.pretrained_path}\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error loading model: {e}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.head(x)\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "\n",
    "        if self.args.no_upsampling:\n",
    "            x = res\n",
    "        else:\n",
    "            x = self.tail(res)\n",
    "        return x\n",
    "\n",
    "    def load_state_dict(self, state_dict, strict=True):\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in state_dict.items():\n",
    "            if name in own_state:\n",
    "                if isinstance(param, nn.Parameter):\n",
    "                    param = param.data\n",
    "                try:\n",
    "                    own_state[name].copy_(param)\n",
    "                except RuntimeError:\n",
    "                    if name.find('tail') == -1:\n",
    "                        raise RuntimeError('While copying the parameter named {}, '\n",
    "                                           'whose dimensions in the model are {} and '\n",
    "                                           'whose dimensions in the checkpoint are {}.'\n",
    "                                           .format(name, own_state[name].size(), param.size()))\n",
    "            elif strict:\n",
    "                if name.find('tail') == -1:\n",
    "                    raise KeyError('unexpected key \"{}\" in state_dict'\n",
    "                                   .format(name))\n",
    "\n",
    "\n",
    "class SegmentationHead(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):\n",
    "        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n",
    "        super().__init__(conv2d, upsampling)\n",
    "\n",
    "@register(\"encoder\")\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args, n_class=100):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.args = args\n",
    "        self.edsr = EDSR(args)\n",
    "        self.segmentation_head = SegmentationHead(in_channels=args.n_feats, out_channels=n_class)\n",
    "        self.out_dim = args.n_feats\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.edsr(x)\n",
    "        logits = self.segmentation_head(x)\n",
    "        B, Class, H, W = logits.shape\n",
    "        logits = logits.permute(0, 2, 3, 1).contiguous().view(B * H * W, Class)\n",
    "        if self.training:\n",
    "            logits = F.gumbel_softmax(logits, tau=1, hard=False)\n",
    "        if not self.training:\n",
    "            logits = F.gumbel_softmax(logits, tau=1, hard=True)\n",
    "        logits = logits.view(B, H, W, Class).permute(0, 3, 1, 2).contiguous()\n",
    "        return x, logits\n",
    "\n",
    "\n",
    "@register('edsr-baseline')\n",
    "def make_encoder_baseline(n_resblocks=16, n_feats=64, res_scale=1, scale=2, no_upsampling=True, rgb_range=1, n_class=100):\n",
    "    url = {\n",
    "        'r16f64x2': 'models/weights/edsr_baseline_x2-1bc95232.pt',\n",
    "        'r16f64x3': 'models/weights/edsr_baseline_x3-abf2a44e.pt',\n",
    "        'r16f64x4': 'models/weights/edsr_baseline_x4-6b446fab.pt',\n",
    "        'r32f256x2': 'models/weights/edsr_x2-0edfb8a3.pt',\n",
    "        'r32f256x3': 'models/weights/edsr_x3-ea3ef2c6.pt',\n",
    "        'r32f256x4': 'models/weights/edsr_x4-4f62e9ef.pt'\n",
    "    }\n",
    "    url_name = 'r{}f{}x{}'.format(n_resblocks, n_feats, scale)\n",
    "    if url_name in url:\n",
    "        url = url[url_name]\n",
    "    else:\n",
    "        url = None\n",
    "    args = Namespace()\n",
    "    args.n_resblocks = n_resblocks\n",
    "    args.n_feats = n_feats\n",
    "    args.res_scale = res_scale\n",
    "    args.scale = [scale]\n",
    "    args.no_upsampling = no_upsampling\n",
    "    args.rgb_range = rgb_range\n",
    "    args.n_colors = 3\n",
    "    args.pretrained_path = url\n",
    "    return Encoder(args, n_class)\n",
    "\n",
    "\n",
    "@register('edsr-large')\n",
    "def make_encoder_large(n_resblocks=32, n_feats=256, res_scale=0.1, scale=2, no_upsampling=True, rgb_range=1, n_class=100):\n",
    "    url = {\n",
    "        'r16f64x2': 'models/weights/edsr_baseline_x2-1bc95232.pt',\n",
    "        'r16f64x3': 'models/weights/edsr_baseline_x3-abf2a44e.pt',\n",
    "        'r16f64x4': 'models/weights/edsr_baseline_x4-6b446fab.pt',\n",
    "        'r32f256x2': 'models/weights/edsr_x2-0edfb8a3.pt',\n",
    "        'r32f256x3': 'models/weights/edsr_x3-ea3ef2c6.pt',\n",
    "        'r32f256x4': 'models/weights/edsr_x4-4f62e9ef.pt'\n",
    "    }\n",
    "    url_name = 'r{}f{}x{}'.format(n_resblocks, n_feats, scale)\n",
    "    if url_name in url:\n",
    "        url = url[url_name]\n",
    "    else:\n",
    "        url = None\n",
    "    args = Namespace()\n",
    "    args.n_resblocks = n_resblocks\n",
    "    args.n_feats = n_feats\n",
    "    args.res_scale = res_scale\n",
    "    args.scale = [scale]\n",
    "    args.no_upsampling = no_upsampling\n",
    "    args.rgb_range = rgb_range\n",
    "    args.n_colors = 3\n",
    "    args.pretrained_path = url\n",
    "    return Encoder(args, n_class)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    network = make_encoder_baseline()\n",
    "    a = torch.rand(1, 3, 48, 48)\n",
    "    print(network(a)[0].shape, network(a)[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: models/weights/edsr_baseline_x2-1bc95232.pt\n",
      "input size:  torch.Size([1, 3, 64, 64]) hr_coord size:  torch.Size([2304, 2]) sr_scale size:  torch.Size([1]) cell_ size:  torch.Size([1, 2304, 2])\n",
      "Encoder feature size: torch.Size([1, 64, 64, 64])\n",
      "2DGS feat shape: torch.Size([1, 8, 64, 64]) 2DGS lr_feat shape: torch.Size([1, 56, 64, 64]) 2DGS logits shape: torch.Size([1, 100, 64, 64])\n",
      "Scale: 2.0\n",
      "HR size: 128 128\n",
      "num_kernels_row: 10 num_kernels_column: 10\n",
      "upsampled_size: (70, 70)\n",
      "unfolded_feature shape: torch.Size([1, 392, 100]) unfolded_logits shape: torch.Size([1, 4900, 100])\n",
      "L: 100\n",
      "unfold_feat shape: torch.Size([100, 8, 7, 7]) unfold_logits shape: torch.Size([100, 100, 7, 7])\n",
      "coords_ shape: torch.Size([49, 2])\n",
      "num_LR_points: 49\n",
      "-----------------fetching_features_from_tensor----------------------\n",
      "image_tensor shape: torch.Size([100, 8, 7, 7])\n",
      "input_coords shape: torch.Size([49, 2])\n",
      "coords shape: torch.Size([49, 2])\n",
      "center_coords_normalized shape: torch.Size([2])\n",
      "coords shape: torch.Size([49, 2])\n",
      "batch_size: 100\n",
      "input_coords_expanded shape: torch.Size([100, 49, 2])\n",
      "y_coords shape: torch.Size([100, 49]) x_coords shape: torch.Size([100, 49])\n",
      "batch_indices shape: torch.Size([100, 1])\n",
      "color_values shape: torch.Size([100, 49, 8]) coords shape: torch.Size([49, 2])\n",
      "-----------------fetching_features_from_tensor----------------------\n",
      "colors_ shape: torch.Size([100, 49, 8]) coords_norm shape: torch.Size([49, 2])\n",
      "-----------------weighted_gaussian_parameters----------------------\n",
      "weighted_sigma_x shape: torch.Size([100, 7, 7]) weighted_sigma_y shape: torch.Size([100, 7, 7]) weighted_opacity shape: torch.Size([100, 7, 7]) weighted_rho shape: torch.Size([100, 7, 7])\n",
      "after reshape and mean\n",
      "weighted_sigma_x shape: torch.Size([49]) weighted_sigma_y shape: torch.Size([49]) weighted_opacity shape: torch.Size([49]) weighted_rho shape: torch.Size([49])\n",
      "-----------------weighted_gaussian_parameters end----------------------\n",
      "sigma_x shape: torch.Size([49, 1, 1]) sigma_y shape: torch.Size([49, 1, 1]) rho shape: torch.Size([49, 1, 1]) weighted_opacity shape: torch.Size([49])\n",
      "covariance shape: torch.Size([49, 1, 1, 2, 2]) inv_covariance shape: torch.Size([49, 1, 1, 2, 2])\n",
      "start shape: torch.Size([1, 1]) end shape: torch.Size([1, 1])\n",
      "base_linspace shape: torch.Size([3]) ax_batch shape: torch.Size([1, 3])\n",
      "ax_batch_expanded_x shape: torch.Size([1, 3, 3]) ax_batch_expanded_y shape: torch.Size([1, 3, 3])\n",
      "xy shape: torch.Size([1, 3, 3, 2])\n",
      "z shape: torch.Size([49, 3, 3])\n",
      "kernel shape: torch.Size([49, 3, 3])\n",
      "pad_h: 11 pad_w: 11\n",
      "padding:  (5, 6, 5, 6)\n",
      "kernel_color_padded shape: torch.Size([49, 8, 14, 14])\n",
      "theta shape: torch.Size([100, 49, 2, 3])\n",
      "grid shape: torch.Size([4900, 14, 14, 2])\n",
      "kernel_color_padded_expanded shape: torch.Size([4900, 8, 14, 14])\n",
      "kernel_color_padded_translated shape: torch.Size([100, 49, 8, 14, 14])\n",
      "colors shape: torch.Size([100, 49, 8])\n",
      "color_values_reshaped shape: torch.Size([100, 49, 8, 1, 1])\n",
      "final_image shape: torch.Size([100, 8, 14, 14])\n",
      "kernel_h: 14 kernel_w: 14\n",
      "final_image shape: torch.Size([1, 1568, 100])\n",
      "final_image shape after folding: torch.Size([1, 8, 140, 140])\n",
      "final_image shape after interpolation: torch.Size([1, 8, 128, 128])\n",
      "lr_feat shape after interpolation: torch.Size([1, 56, 128, 128])\n",
      "final_image shape after concatenation: torch.Size([1, 64, 128, 128])\n",
      "-----------------query_rgb end----------------------\n",
      "-----------------augmentation start----------------------\n",
      "coef shape: torch.Size([1, 256, 128, 128]) freq shape: torch.Size([1, 256, 128, 128]) feat_coord shape: torch.Size([1, 2, 64, 64]) hr coord shape: torch.Size([1, 2304, 2])\n",
      "q_coef shape: torch.Size([1, 2304, 256]) q_freq shape: torch.Size([1, 2304, 256]) q_coord shape: torch.Size([1, 2304, 2])\n",
      "q coef shape:  torch.Size([1, 2304, 256]) q freq shape:  torch.Size([1, 2304, 256]) inp shape:  torch.Size([1, 2304, 256])\n",
      "model output shape:  torch.Size([1, 2304, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Workspace\\2d_gaussian_SR\\models\\edsr.py:123: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(args.pretrained_path)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import models\n",
    "from models import register\n",
    "\n",
    "from utils import to_pixel_samples\n",
    "\n",
    "\n",
    "def default_conv(in_channels, out_channels, kernel_size, bias=True):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size // 2), bias=bias)\n",
    "\n",
    "\n",
    "def make_coord(shape, ranges=None, flatten=True):\n",
    "    \"\"\" Make coordinates at grid centers.\"\"\"\n",
    "    coord_seqs = []\n",
    "    for i, n in enumerate(shape):\n",
    "        if ranges is None:\n",
    "            v0, v1 = -1, 1\n",
    "        else:\n",
    "            v0, v1 = ranges[i]\n",
    "        r = (v1 - v0) / (2 * n)\n",
    "        seq = v0 + r + (2 * r) * torch.arange(n).float()\n",
    "        coord_seqs.append(seq)\n",
    "    ret = torch.stack(torch.meshgrid(*coord_seqs), dim=-1)\n",
    "    if flatten:\n",
    "        ret = ret.view(-1, ret.shape[-1])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def generate_meshgrid(height, width):\n",
    "    \"\"\"\n",
    "    Generate a meshgrid of coordinates for a given image dimensions.\n",
    "    Args:\n",
    "        height (int): Height of the image.\n",
    "        width (int): Width of the image.\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape [height * width, 2] containing the (x, y) coordinates for each pixel in the image.\n",
    "    \"\"\"\n",
    "    # Generate all pixel coordinates for the given image dimensions\n",
    "    y_coords, x_coords = torch.arange(0, height), torch.arange(0, width)\n",
    "    # Create a grid of coordinates\n",
    "    yy, xx = torch.meshgrid(y_coords, x_coords)\n",
    "    # Flatten and stack the coordinates to obtain a list of (x, y) pairs\n",
    "    all_coords = torch.stack([xx.flatten(), yy.flatten()], dim=1)\n",
    "    return all_coords\n",
    "\n",
    "\n",
    "def fetching_features_from_tensor(image_tensor, input_coords):\n",
    "    \"\"\"\n",
    "    Extracts pixel values from a tensor of images at specified coordinate locations.\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): A 4D tensor of shape [batch, channel, height, width] representing a batch of images.\n",
    "        input_coords (torch.Tensor): A 2D tensor of shape [N, 2] containing the (x, y) coordinates at which to extract pixel values.\n",
    "    Returns:\n",
    "        color_values (torch.Tensor): A 3D tensor of shape [batch, N, channel] containing the pixel values at the specified coordinates.\n",
    "        coords (torch.Tensor): A 2D tensor of shape [N, 2] containing the normalized coordinates in the range [-1, 1].\n",
    "    \"\"\"\n",
    "    # Normalize pixel coordinates to [-1, 1] range\n",
    "    print(\"-----------------fetching_features_from_tensor----------------------\")\n",
    "    print(\"image_tensor shape:\", image_tensor.shape)\n",
    "    input_coords = input_coords.to(image_tensor.device)\n",
    "    print(\"input_coords shape:\", input_coords.shape)\n",
    "    coords = input_coords / torch.tensor([image_tensor.shape[-2], image_tensor.shape[-1]],\n",
    "                                         device=image_tensor.device).float()\n",
    "    print(\"coords shape:\", coords.shape)\n",
    "    center_coords_normalized = torch.tensor([0.5, 0.5], device=image_tensor.device).float()\n",
    "    print(\"center_coords_normalized shape:\", center_coords_normalized.shape)\n",
    "    coords = (center_coords_normalized - coords) * 2.0\n",
    "    print(\"coords shape:\", coords.shape)\n",
    "    # Fetching the colour of the pixels in each coordinates\n",
    "    batch_size = image_tensor.shape[0]\n",
    "    print(\"batch_size:\", batch_size)\n",
    "    input_coords_expanded = input_coords.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "    print(\"input_coords_expanded shape:\", input_coords_expanded.shape)\n",
    "    y_coords = input_coords_expanded[..., 0].long()\n",
    "    x_coords = input_coords_expanded[..., 1].long()\n",
    "    print(\"y_coords shape:\", y_coords.shape, \"x_coords shape:\", x_coords.shape)\n",
    "    batch_indices = torch.arange(batch_size).view(-1, 1).to(input_coords.device)\n",
    "    print(\"batch_indices shape:\", batch_indices.shape)\n",
    "    color_values = image_tensor[batch_indices, :, x_coords, y_coords]\n",
    "    print(\"color_values shape:\", color_values.shape, \"coords shape:\", coords.shape)\n",
    "    print(\"-----------------fetching_features_from_tensor----------------------\")\n",
    "    return color_values, coords\n",
    "\n",
    "\n",
    "def extract_patch(image, center, radius, padding_mode='constant'):\n",
    "    \"\"\"\n",
    "    Extract a patch from an image with the specified center and radius.\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image of shape [batch_size, channels, height, width].\n",
    "        center (tuple): Coordinates (y, x) of the patch center.\n",
    "        radius (int): Radius of the patch.\n",
    "        padding_mode (str, optional): Padding mode, can be 'constant', 'reflect', 'replicate', or 'circular'. Default is 'constant'.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Extracted patch of shape [batch_size, channels, 2 * radius, 2 * radius].\n",
    "    \"\"\"\n",
    "    height, width = image.shape[-2:]\n",
    "\n",
    "    # Convert center coordinates to integers\n",
    "    center_y, center_x = int(round(center[0])), int(round(center[1]))\n",
    "\n",
    "    # Calculate patch boundaries\n",
    "    top = center_y - radius\n",
    "    bottom = center_y + radius\n",
    "    left = center_x - radius\n",
    "    right = center_x + radius\n",
    "\n",
    "    # Check if boundaries are out of image bounds\n",
    "    top_padding = max(0, -top)\n",
    "    bottom_padding = max(0, bottom - height)\n",
    "    left_padding = max(0, -left)\n",
    "    right_padding = max(0, right - width)\n",
    "\n",
    "    # Pad the image\n",
    "    padded_image = torch.nn.functional.pad(image, (left_padding, right_padding, top_padding, bottom_padding),\n",
    "                                           mode=padding_mode)\n",
    "\n",
    "    # Extract the patch\n",
    "    patch = padded_image[..., top_padding:top_padding + 2 * radius, left_padding:left_padding + 2 * radius]\n",
    "\n",
    "    return patch\n",
    "\n",
    "\n",
    "@register('gaussian-splatter')\n",
    "class GaussianSplatter(nn.Module):\n",
    "    \"\"\"A module that applies 2D Gaussian splatting to input features.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder_spec, dec_spec, kernel_size, hidden_dim=256, unfold_row=7, unfold_column=7,\n",
    "                 num_points=100):\n",
    "        \"\"\"\n",
    "        Initialize the 2D Gaussian Splatter module.\n",
    "        Args:\n",
    "            kernel_size (int): The size of the kernel to convert rasterization.\n",
    "            unfold_row (int): The number of points in the row dimension of the Gaussian grid.\n",
    "            unfold_column (int): The number of points in the column dimension of the Gaussian grid.\n",
    "        \"\"\"\n",
    "        super(GaussianSplatter, self).__init__()\n",
    "        self.encoder = models.make(encoder_spec)\n",
    "        self.feat, self.logits = None, None\n",
    "\n",
    "        self.coef = nn.Conv2d(self.encoder.out_dim, hidden_dim, 3, padding=1)\n",
    "        self.freq = nn.Conv2d(self.encoder.out_dim, hidden_dim, 3, padding=1)\n",
    "        self.phase = nn.Linear(2, hidden_dim // 2, bias=False)\n",
    "        self.dec = models.make(dec_spec, args={'in_dim': hidden_dim})\n",
    "\n",
    "        # Key parameter in 2D Gaussian Splatter\n",
    "        self.kernel_size = kernel_size\n",
    "        self.row = unfold_row\n",
    "        self.column = unfold_column\n",
    "        self.num_points = num_points\n",
    "\n",
    "        # Initialize Trainable Parameters\n",
    "        sigma_x, sigma_y = torch.meshgrid(torch.linspace(0.2, 3.0, 10), torch.linspace(0.2, 3.0, 10))\n",
    "        self.sigma_x = sigma_x.reshape(-1)\n",
    "        self.sigma_y = sigma_y.reshape(-1)\n",
    "        self.opacity = torch.sigmoid(torch.ones(self.num_points, 1, requires_grad=True))\n",
    "        self.rho = torch.clamp(torch.zeros(self.num_points, 1, requires_grad=True), min=-1, max=1)\n",
    "        self.sigma_x = nn.Parameter(self.sigma_x)  # Standard deviation in x-axis\n",
    "        self.sigma_y = nn.Parameter(self.sigma_y)  # Standard deviation in y-axis\n",
    "        self.opacity = nn.Parameter(self.opacity)  # Transparency of feature, shape=[num_points, 1]\n",
    "        self.rho = nn.Parameter(self.rho)\n",
    "\n",
    "    def weighted_gaussian_parameters(self, logits):\n",
    "        \"\"\"\n",
    "        Computes weighted Gaussian parameters based on logits and the Gaussian kernel parameters (sigma_x, sigma_y, opacity).\n",
    "        The logits tensor is used as a weight to compute a weighted sum of the Gaussian kernel parameters for each spatial\n",
    "        location across the batch dimension. The resulting weighted parameters are then averaged across the batch dimension.\n",
    "        Args:\n",
    "            logits (torch.Tensor): Logits tensor of shape [batch, class, height, width].\n",
    "        Returns:\n",
    "            tuple: A tuple containing the weighted Gaussian parameters:\n",
    "                - weighted_sigma_x (torch.Tensor): Tensor of shape [height * width] representing the weighted x-axis standard deviations.\n",
    "                - weighted_sigma_y (torch.Tensor): Tensor of shape [height * width] representing the weighted y-axis standard deviations.\n",
    "                - weighted_opacity (torch.Tensor): Tensor of shape [height * width] representing the weighted opacities.\n",
    "        Description:\n",
    "            This function computes weighted Gaussian parameters based on the input tensor, logits, and the provided Gaussian kernel parameters (sigma_x, sigma_y, and opacity). The logits tensor is used as a weight to compute a weighted sum of the Gaussian kernel parameters for each spatial location (height and width) across the batch dimension. The resulting weighted parameters are then averaged across the batch dimension, yielding tensors of shape [height * width] for the weighted sigma_x, sigma_y, and opacity.\n",
    "        \"\"\"\n",
    "        batch_size, num_classes, height, width = logits.size()\n",
    "        logits = logits.permute(0, 2, 3, 1)  # Reshape logits to [batch, height, width, class] = [100, 7, 7, 100]\n",
    "\n",
    "        # Compute weighted sum of Gaussian parameters across class dimension\n",
    "        weighted_sigma_x = (logits * self.sigma_x.unsqueeze(0).unsqueeze(0).unsqueeze(0)).sum(dim=-1)\n",
    "        weighted_sigma_y = (logits * self.sigma_y.unsqueeze(0).unsqueeze(0).unsqueeze(0)).sum(dim=-1)\n",
    "        weighted_opacity = (logits * self.opacity[:, 0].unsqueeze(0).unsqueeze(0).unsqueeze(0)).sum(dim=-1)\n",
    "        weighted_rho = (logits * self.rho[:, 0].unsqueeze(0).unsqueeze(0).unsqueeze(0)).sum(dim=-1)\n",
    "        print(\"-----------------weighted_gaussian_parameters----------------------\")\n",
    "        print(\"weighted_sigma_x shape:\", weighted_sigma_x.shape, \"weighted_sigma_y shape:\", weighted_sigma_y.shape, \"weighted_opacity shape:\", weighted_opacity.shape, \"weighted_rho shape:\", weighted_rho.shape)\n",
    "\n",
    "        # Reshape and average across batch dimension\n",
    "        weighted_sigma_x = weighted_sigma_x.reshape(batch_size, -1).mean(dim=0)\n",
    "        weighted_sigma_y = weighted_sigma_y.reshape(batch_size, -1).mean(dim=0)\n",
    "        weighted_opacity = weighted_opacity.reshape(batch_size, -1).mean(dim=0)\n",
    "        weighted_rho = weighted_rho.reshape(batch_size, -1).mean(dim=0)\n",
    "        print(\"after reshape and mean\")\n",
    "        print(\"weighted_sigma_x shape:\", weighted_sigma_x.shape, \"weighted_sigma_y shape:\", weighted_sigma_y.shape, \"weighted_opacity shape:\", weighted_opacity.shape, \"weighted_rho shape:\", weighted_rho.shape)\n",
    "        print(\"-----------------weighted_gaussian_parameters end----------------------\")\n",
    "        return weighted_sigma_x, weighted_sigma_y, weighted_opacity, weighted_rho\n",
    "\n",
    "    def gen_feat(self, inp):\n",
    "        \"\"\"Generate feature and logits by encoder.\"\"\"\n",
    "        self.inp = inp\n",
    "        self.feat, self.logits = self.encoder(inp)\n",
    "        self.feat_coord = make_coord(inp.shape[-2:], flatten=False).cuda().permute(2, 0, 1) \\\n",
    "            .unsqueeze(0).expand(inp.shape[0], 2, *inp.shape[-2:])\n",
    "        return self.feat, self.logits\n",
    "\n",
    "    def query_rgb(self, coord, scale, cell=None):\n",
    "        \"\"\"\n",
    "        Continuous sampling through 2D Gaussian Splatting.\n",
    "        Args:\n",
    "            coord (torch.Tensor): [Batch, Sample_q, 2]. The normalized coordinates of HR space (of range [-1, 1]).\n",
    "            cell (torch.Tensor): [Batch, Sample_q, 2]. The normalized cell size of HR space.\n",
    "            scale (torch.Tensor): [Batch]. The magnification scale of super-resolution. (1, 4) during training.\n",
    "        Returns:\n",
    "            torch.Tensor: The output features after Gaussian splatting, of the same shape as the input.\n",
    "        \"\"\"\n",
    "        # 1. Get LR feature and logits\n",
    "        print('Encoder feature size:', self.feat.shape)\n",
    "        feat, lr_feat, logits = self.feat[:, :8, :, :], self.feat[:, 8:, :, :], self.logits  # Channel decoupling\n",
    "        print(\"2DGS feat shape:\", feat.shape, \"2DGS lr_feat shape:\", lr_feat.shape, \"2DGS logits shape:\", logits.shape)\n",
    "        feat_size, feat_device = feat.shape, feat.device\n",
    "\n",
    "        # 2. Calculate the high-resolution image size\n",
    "        scale = float(scale[0])\n",
    "        print(\"Scale:\", scale)\n",
    "        hr_h = round(feat.shape[-2] * scale)  # shape: [batch size]\n",
    "        hr_w = round(feat.shape[-1] * scale)\n",
    "        print(\"HR size:\", hr_h, hr_w)\n",
    "        \n",
    "        # 3. Unfold the feature / logits to many small patches to avoid extreme GPU memory consumption\n",
    "        num_kernels_row = math.ceil(feat_size[-2] / self.row)\n",
    "        num_kernels_column = math.ceil(feat_size[-1] / self.column)\n",
    "        print(\"num_kernels_row:\", num_kernels_row, \"num_kernels_column:\", num_kernels_column)\n",
    "        upsampled_size = (num_kernels_row * self.row, num_kernels_column * self.column)\n",
    "        print(\"upsampled_size:\", upsampled_size)\n",
    "        upsampled_inp = F.interpolate(feat, size=upsampled_size, mode='bicubic', align_corners=False)\n",
    "        upsampled_logits = F.interpolate(logits, size=upsampled_size, mode='bicubic', align_corners=False)\n",
    "        unfold = nn.Unfold(kernel_size=(self.row, self.column), stride=(self.row, self.column))\n",
    "        unfolded_feature = unfold(upsampled_inp)\n",
    "        unfolded_logits = unfold(upsampled_logits)\n",
    "        print(\"unfolded_feature shape:\", unfolded_feature.shape, \"unfolded_logits shape:\", unfolded_logits.shape)\n",
    "        # Unfolded_feature dimension becomes [Batch, C*K*K, L], where L is the number of columns after unfolding\n",
    "        L = unfolded_feature.shape[-1]\n",
    "        print(\"L:\", L)\n",
    "        unfolded_feature_reshaped = unfolded_feature.transpose(1, 2). \\\n",
    "            reshape(feat_size[0] * L, feat_size[1], self.row, self.column)\n",
    "        unfold_feat = unfolded_feature_reshaped  # shape: [num of patch * batch, channel, self.row, self.column]\n",
    "        unfolded_logits_reshaped = unfolded_logits.transpose(1, 2). \\\n",
    "            reshape(logits.shape[0] * L, logits.shape[1], self.row, self.column)\n",
    "        unfold_logits = unfolded_logits_reshaped  # shape: [num of patch * batch, channel, self.row, self.column]\n",
    "        print(\"unfold_feat shape:\", unfold_feat.shape, \"unfold_logits shape:\", unfold_logits.shape)\n",
    "        \n",
    "        # 4. Generate colors_(features) and coords_norm\n",
    "        coords_ = generate_meshgrid(unfold_feat.shape[-2], unfold_feat.shape[-1])  # 메쉬 그리드 생성\n",
    "        print(\"coords_ shape:\", coords_.shape)\n",
    "        num_LR_points = unfold_feat.shape[-2] * unfold_feat.shape[-1]  # 저해상도 포인트 수 계산\n",
    "        print(\"num_LR_points:\", num_LR_points)\n",
    "        colors_, coords_norm = fetching_features_from_tensor(unfold_feat, coords_)  # 텐서에서 특징 추출\n",
    "        print(\"colors_ shape:\", colors_.shape, \"coords_norm shape:\", coords_norm.shape)\n",
    "\n",
    "        # 5. Rasterization: Generating grid\n",
    "        # 5.1. Spread Gaussian points over the whole feature map\n",
    "        batch_size, channel, _, _ = unfold_feat.shape  # 배치 크기와 채널 수 가져오기\n",
    "        weighted_sigma_x, weighted_sigma_y, weighted_opacity, weighted_rho = self.weighted_gaussian_parameters(\n",
    "            unfold_logits)  # 가중치가 적용된 가우시안 파라미터 계산\n",
    "        sigma_x = weighted_sigma_x.view(num_LR_points, 1, 1)  # 가우시안 x축 표준편차\n",
    "        sigma_y = weighted_sigma_y.view(num_LR_points, 1, 1)  # 가우시안 y축 표준편차\n",
    "        rho = weighted_rho.view(num_LR_points, 1, 1)  # 가우시안 상관계수\n",
    "        print(\"sigma_x shape:\", sigma_x.shape, \"sigma_y shape:\", sigma_y.shape, \"rho shape:\", rho.shape, \"weighted_opacity shape:\", weighted_opacity.shape)\n",
    "        # 5.2. Gaussian expression\n",
    "        covariance = torch.stack(\n",
    "            [torch.stack([sigma_x ** 2 + 1e-5, rho * sigma_x * sigma_y], dim=-1),\n",
    "             torch.stack([rho * sigma_x * sigma_y, sigma_y ** 2 + 1e-5], dim=-1)], dim=-2\n",
    "        )  # 공분산 행렬 계산\n",
    "        inv_covariance = torch.inverse(covariance).to(feat_device)  # 공분산 행렬의 역행렬 계산\n",
    "        print(\"covariance shape:\", covariance.shape, \"inv_covariance shape:\", inv_covariance.shape)\n",
    "        \n",
    "        # 5.3. Choosing a broad range for the distribution [-5,5] to avoid any clipping\n",
    "        start = torch.tensor([-5.0], device=feat_device).view(-1, 1)  # 시작 범위 설정\n",
    "        end = torch.tensor([5.0], device=feat_device).view(-1, 1)  # 끝 범위 설정\n",
    "        print(\"start shape:\", start.shape, \"end shape:\", end.shape)\n",
    "        base_linspace = torch.linspace(0, 1, steps=self.kernel_size, device=feat_device)  # 선형 공간 생성\n",
    "        ax_batch = start + (end - start) * base_linspace  # 배치 축 생성\n",
    "        print(\"base_linspace shape:\", base_linspace.shape, \"ax_batch shape:\", ax_batch.shape)\n",
    "        # Expanding dims for broadcasting\n",
    "        ax_batch_expanded_x = ax_batch.unsqueeze(-1).expand(-1, -1, self.kernel_size)  # x축 확장\n",
    "        ax_batch_expanded_y = ax_batch.unsqueeze(1).expand(-1, self.kernel_size, -1)  # y축 확장\n",
    "        print(\"ax_batch_expanded_x shape:\", ax_batch_expanded_x.shape, \"ax_batch_expanded_y shape:\", ax_batch_expanded_y.shape)\n",
    "\n",
    "        # 5.4. Creating a batch-wise meshgrid using broadcasting\n",
    "        xx, yy = ax_batch_expanded_x, ax_batch_expanded_y  # 메쉬 그리드 생성\n",
    "        xy = torch.stack([xx, yy], dim=-1)  # x, y 좌표 스택\n",
    "        print(\"xy shape:\", xy.shape)\n",
    "        z = torch.einsum('b...i,b...ij,b...j->b...', xy, -0.5 * inv_covariance, xy)  # 가우시안 계산\n",
    "        print(\"z shape:\", z.shape)\n",
    "        kernel = torch.exp(z) / (2 * torch.tensor(np.pi, device=feat_device) *\n",
    "                     torch.sqrt(torch.det(covariance)).to(feat_device).view(num_LR_points, 1, 1))  # 커널 계산\n",
    "        print(\"kernel shape:\", kernel.shape)\n",
    "        kernel_max_1, _ = kernel.max(dim=-1, keepdim=True)  # 첫 번째 최대값 찾기\n",
    "        kernel_max_2, _ = kernel_max_1.max(dim=-2, keepdim=True)  # 두 번째 최대값 찾기\n",
    "        kernel_normalized = kernel / kernel_max_2  # 커널 정규화\n",
    "        kernel_reshaped = kernel_normalized.repeat(1, channel, 1).contiguous(). \\\n",
    "            view(num_LR_points * channel, self.kernel_size, self.kernel_size)  # 커널 재구성\n",
    "        kernel_color = kernel_reshaped.unsqueeze(0).reshape(num_LR_points, channel, self.kernel_size, self.kernel_size)  # 커널 색상\n",
    "\n",
    "        # 5.5. Adding padding to make kernel size equal to the image size\n",
    "        pad_h = round(unfold_feat.shape[-2] * scale) - self.kernel_size  # 패딩 높이 계산\n",
    "        pad_w = round(unfold_feat.shape[-1] * scale) - self.kernel_size  # 패딩 너비 계산\n",
    "        print(\"pad_h:\", pad_h, \"pad_w:\", pad_w)\n",
    "        if pad_h < 0 or pad_w < 0:  # 패딩 크기 확인\n",
    "            raise ValueError(\"Kernel size should be smaller or equal to the image size.\")  # 오류 발생\n",
    "        padding = (pad_w // 2, pad_w // 2 + pad_w % 2, pad_h // 2, pad_h // 2 + pad_h % 2)  # 패딩 값 설정\n",
    "        kernel_color_padded = torch.nn.functional.pad(kernel_color, padding, \"constant\", 0)  # 패딩 적용\n",
    "        print(\"padding: \", padding)\n",
    "        print(\"kernel_color_padded shape:\", kernel_color_padded.shape)\n",
    "\n",
    "        # 5.6. Create a batch of 2D affine matrices\n",
    "        b, c, h, w = kernel_color_padded.shape  # num_LR_points, channel, hr_h, hr_w\n",
    "        theta = torch.zeros(batch_size, b, 2, 3, dtype=torch.float32, device=feat_device)  # 2D 아핀 행렬 생성\n",
    "        theta[:, :, 0, 0] = 1.0  # x축 스케일 설정\n",
    "        theta[:, :, 1, 1] = 1.0  # y축 스케일 설정\n",
    "        theta[:, :, :, 2] = coords_norm  # 좌표 설정\n",
    "        print(\"theta shape:\", theta.shape)\n",
    "        grid = F.affine_grid(theta.view(-1, 2, 3), size=[batch_size * b, c, h, w], align_corners=True).contiguous()  # 그리드 생성\n",
    "        print(\"grid shape:\", grid.shape)\n",
    "        kernel_color_padded_expanded = kernel_color_padded.repeat(batch_size, 1, 1, 1).contiguous()  # 패딩 확장\n",
    "        print(\"kernel_color_padded_expanded shape:\", kernel_color_padded_expanded.shape)\n",
    "        kernel_color_padded_translated = F.grid_sample(kernel_color_padded_expanded.contiguous(), grid.contiguous(),\n",
    "                                   align_corners=True)  # 그리드 샘플링\n",
    "        kernel_color_padded_translated = kernel_color_padded_translated.view(batch_size, b, c, h, w)  # 재구성\n",
    "        print(\"kernel_color_padded_translated shape:\", kernel_color_padded_translated.shape)\n",
    "\n",
    "        # 6. Apply Gaussian splatting\n",
    "        # colors_.shape = [batch, num_LR_points, channel], colors.shape = [batch, num_LR_points, channel]\n",
    "        colors = colors_ * weighted_opacity.to(feat_device).unsqueeze(-1).expand(batch_size, -1, -1)  # 색상 적용\n",
    "        print(\"colors shape:\", colors.shape)\n",
    "        color_values_reshaped = colors.unsqueeze(-1).unsqueeze(-1)  # 색상 재구성\n",
    "        print(\"color_values_reshaped shape:\", color_values_reshaped.shape)\n",
    "        final_image_layers = color_values_reshaped * kernel_color_padded_translated  # 최종 이미지 레이어 생성\n",
    "        final_image = final_image_layers.sum(dim=1)  # 최종 이미지 합산\n",
    "        final_image = torch.clamp(final_image, 0, 1)  # 이미지 클램프\n",
    "        print(\"final_image shape:\", final_image.shape)\n",
    "\n",
    "        # 7. Fold the input back to the original size\n",
    "        # Calculate the number of kernels needed to cover each dimension.\n",
    "        kernel_h, kernel_w = round(self.row * scale), round(self.column * scale)  # 커널 크기 계산\n",
    "        print(\"kernel_h:\", kernel_h, \"kernel_w:\", kernel_w)\n",
    "        fold = nn.Fold(output_size=(kernel_h * num_kernels_row, kernel_w * num_kernels_column),\n",
    "                   kernel_size=(kernel_h, kernel_w), stride=(kernel_h, kernel_w))  # 폴드 설정\n",
    "        final_image = final_image.reshape(feat_size[0], L, feat_size[1] * kernel_h * kernel_w).transpose(1, 2)  # 이미지 재구성\n",
    "        print(\"final_image shape:\", final_image.shape)\n",
    "        final_image = fold(final_image)  # 폴드 적용\n",
    "        print(\"final_image shape after folding:\", final_image.shape)\n",
    "        final_image = F.interpolate(final_image, size=(hr_h, hr_w), mode='bicubic', align_corners=False)  # 이미지 보간\n",
    "        print(\"final_image shape after interpolation:\", final_image.shape)\n",
    "        # Combine channel\n",
    "        lr_feat = F.interpolate(lr_feat, size=(hr_h, hr_w), mode='bicubic', align_corners=False)  # 저해상도 특징 보간\n",
    "        print(\"lr_feat shape after interpolation:\", lr_feat.shape)\n",
    "        final_image = torch.concat((final_image, lr_feat), dim=1)  # 채널 결합\n",
    "        print(\"final_image shape after concatenation:\", final_image.shape)\n",
    "        print(\"-----------------query_rgb end----------------------\")\n",
    "        print(\"-----------------augmentation start----------------------\")\n",
    "        # 8. Augmentation (Useful for improving out-of-distribution scale performance)\n",
    "        coef = self.coef(final_image)  # 계수 계산\n",
    "        freq = self.freq(final_image)  # 주파수 계산\n",
    "        feat_coord = self.feat_coord  # 특징 좌표 설정\n",
    "        print(\"coef shape:\", coef.shape, \"freq shape:\", freq.shape, \"feat_coord shape:\", feat_coord.shape, \"hr coord shape:\", coord.shape)\n",
    "        coord_ = coord.clone()  # 좌표 복제\n",
    "        q_coef = F.grid_sample(coef, coord_.flip(-1).unsqueeze(1), mode='nearest', align_corners=False)[:, :, 0, :] \\\n",
    "            .permute(0, 2, 1)  # 계수 샘플링\n",
    "        q_freq = F.grid_sample(freq, coord_.flip(-1).unsqueeze(1), mode='nearest', align_corners=False)[:, :, 0, :] \\\n",
    "            .permute(0, 2, 1)  # 주파수 샘플링\n",
    "         \n",
    "        feat_coord = feat_coord.to(coord_.device)  # 특징 좌표 디바이스 설정\n",
    "        q_coord = F.grid_sample(feat_coord, coord_.flip(-1).unsqueeze(1), mode='nearest', align_corners=False)[:, :, 0,\n",
    "              :] \\\n",
    "            .permute(0, 2, 1)  # 좌표 샘플링\n",
    "        print(\"q_coef shape:\", q_coef.shape, \"q_freq shape:\", q_freq.shape, \"q_coord shape:\", q_coord.shape)   \n",
    "        \n",
    "        rel_coord = coord - q_coord  # 상대 좌표 계산\n",
    "        rel_coord[:, :, 0] *= feat.shape[-2]  # 상대 좌표 x축 계산\n",
    "        rel_coord[:, :, 1] *= feat.shape[-1]  # 상대 좌표 y축 계산\n",
    "        rel_cell = cell.clone()  # 셀 복제\n",
    "        rel_cell[:, :, 0] *= feat.shape[-2]  # 상대 셀 x축 계산\n",
    "        rel_cell[:, :, 1] *= feat.shape[-1]  # 상대 셀 y축 계산\n",
    "        bs, q = coord.shape[:2]  # 배치 크기와 쿼리 수 설정\n",
    "        q_freq = torch.stack(torch.split(q_freq, 2, dim=-1), dim=-1)  # 주파수 스택\n",
    "        q_freq = torch.mul(q_freq, rel_coord.unsqueeze(-1))  # 주파수 곱셈\n",
    "        q_freq = torch.sum(q_freq, dim=-2)  # 주파수 합산\n",
    "        q_freq += self.phase(rel_cell.view((bs * q, -1))).view(bs, q, -1)  # 주파수 계산\n",
    "        q_freq = torch.cat((torch.cos(np.pi * q_freq), torch.sin(np.pi * q_freq)), dim=-1)  # 주파수 결합\n",
    "\n",
    "        inp = torch.mul(q_coef, q_freq)  # 입력 계산\n",
    "        print(\"q coef shape: \", q_coef.shape, \"q freq shape: \", q_freq.shape, \"inp shape: \", inp.shape)\n",
    "        pred = self.dec(inp.contiguous().view(bs * q, -1)).view(bs, q, -1)  # 예측값 계산\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def forward(self, inp, coord, scale, cell=None):\n",
    "        self.gen_feat(inp)\n",
    "        return self.query_rgb(coord, scale, cell)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # A simple example of implementing class GaussianSplatter\n",
    "    model = GaussianSplatter(encoder_spec={\"name\": \"edsr-baseline\", \"args\": {\"no_upsampling\": True}},\n",
    "                             dec_spec={\"name\": \"mlp\", \"args\": {\"out_dim\": 3, \"hidden_list\": [256, 256, 256, 256]}},\n",
    "                             kernel_size=3)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input = torch.rand(1, 3, 64, 64)\n",
    "    # input = input.to(device)\n",
    "    sr_scale = 2\n",
    "    hr_coord, hr_rgb = to_pixel_samples(\n",
    "        F.interpolate(input, size=(round(input.shape[-2] * sr_scale), round(input.shape[-1] * sr_scale)),\n",
    "                      mode='bicubic', align_corners=False))\n",
    "    v0_x, v1_x, v0_y, v1_y = -1, 1, -1, 1\n",
    "    nx, ny = round(input.shape[-2] * sr_scale), round(input.shape[-1] * sr_scale)\n",
    "\n",
    "    x = ((hr_coord[..., 0] - v0_x) / (v1_x - v0_x) * 2 * (nx - 1) / 2).round().long()\n",
    "    y = ((hr_coord[..., 1] - v0_y) / (v1_y - v0_y) * 2 * (ny - 1) / 2).round().long()\n",
    "    restored_coords = torch.stack([x, y], dim=-1)\n",
    "\n",
    "    sample_lst = np.random.choice(len(hr_coord), 2304, replace=False)\n",
    "    hr_coord = hr_coord[sample_lst]\n",
    "    hr_rgb = hr_rgb[sample_lst]\n",
    "    cell_ = torch.ones_like(hr_coord.unsqueeze(0))\n",
    "    cell_[:, 0] *= 2 / nx\n",
    "    cell_[:, 1] *= 2 / ny\n",
    "    sr_scale = 2 * torch.ones(1)\n",
    "    # print(f\"Input device: {input.device}\")\n",
    "    print(\"input size: \", input.shape, \"hr_coord size: \", hr_coord.shape, \"sr_scale size: \", sr_scale.shape, \"cell_ size: \", cell_.shape)\n",
    "    print(\"model output shape: \", model(input, hr_coord.unsqueeze(0), sr_scale, cell_).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (act): GELU(approximate='none')\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): GELU(approximate='none')\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): GELU(approximate='none')\n",
      "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (7): GELU(approximate='none')\n",
      "    (8): Linear(in_features=256, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from models import register\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, hidden_list, act='gelu'):\n",
    "        super().__init__()\n",
    "        if act is None:\n",
    "            self.act = None\n",
    "        elif act.lower() == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif act.lower() == 'gelu':\n",
    "            self.act = nn.GELU()\n",
    "        else:\n",
    "            assert False, f'activation {act} is not supported'\n",
    "        layers = []\n",
    "        lastv = in_dim\n",
    "        for hidden in hidden_list:\n",
    "            layers.append(nn.Linear(lastv, hidden))\n",
    "            if self.act:\n",
    "                layers.append(self.act)\n",
    "            lastv = hidden\n",
    "        layers.append(nn.Linear(lastv, out_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape[:-1]\n",
    "        x = self.layers(x.view(-1, x.shape[-1]))\n",
    "        return x.view(*shape, -1)\n",
    "\n",
    "\n",
    "# Create an instance of the MLP class\n",
    "mlp_model = MLP(in_dim=256, out_dim=3, hidden_list=[256, 256, 256, 256], act='gelu')\n",
    "\n",
    "# Print the structure of the MLP model\n",
    "print(mlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix 1:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Matrix 2:\n",
      "tensor([[4, 3],\n",
      "        [2, 1]])\n",
      "Matrix Multiplication:\n",
      "tensor([[4, 6],\n",
      "        [6, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Create two 2x2 matrices\n",
    "matrix1 = torch.tensor([[1, 2], [3, 4]])\n",
    "matrix2 = torch.tensor([[4, 3], [2, 1]])\n",
    "\n",
    "print(\"Matrix 1:\")\n",
    "print(matrix1)\n",
    "\n",
    "print(\"Matrix 2:\")\n",
    "print(matrix2)\n",
    "\n",
    "mul = torch.mul(matrix1, matrix2)\n",
    "print(\"Matrix Multiplication:\")\n",
    "print(mul)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
