{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: models/weights/edsr_baseline_x2-1bc95232.pt\n",
      "torch.Size([1, 64, 48, 48]) torch.Size([1, 100, 48, 48])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/g8qdvht528xc9srh5br0f6jc0000gn/T/ipykernel_30310/3632505019.py:123: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(args.pretrained_path)\n"
     ]
    }
   ],
   "source": [
    "# modified from: https://github.com/thstkdgus35/EDSR-PyTorch\n",
    "import math\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import register\n",
    "\n",
    "\n",
    "def default_conv(in_channels, out_channels, kernel_size, bias=True):\n",
    "    return nn.Conv2d(\n",
    "        in_channels, out_channels, kernel_size,\n",
    "        padding=(kernel_size // 2), bias=bias)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, conv, n_feats, kernel_size,\n",
    "            bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n",
    "\n",
    "        super(ResBlock, self).__init__()\n",
    "        m = []\n",
    "        for i in range(2):\n",
    "            m.append(conv(n_feats, n_feats, kernel_size, bias=bias))\n",
    "            if bn:\n",
    "                m.append(nn.BatchNorm2d(n_feats))\n",
    "            if i == 0:\n",
    "                m.append(act)\n",
    "\n",
    "        self.body = nn.Sequential(*m)\n",
    "        self.res_scale = res_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x).mul(self.res_scale)\n",
    "        res += x\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "class Upsampler(nn.Sequential):\n",
    "    def __init__(self, conv, scale, n_feats, bn=False, act=False, bias=True):\n",
    "\n",
    "        m = []\n",
    "        if (scale & (scale - 1)) == 0:\n",
    "            for _ in range(int(math.log(scale, 2))):\n",
    "                m.append(conv(n_feats, 4 * n_feats, 3, bias))\n",
    "                m.append(nn.PixelShuffle(2))\n",
    "                if bn:\n",
    "                    m.append(nn.BatchNorm2d(n_feats))\n",
    "                if act == 'relu':\n",
    "                    m.append(nn.ReLU(True))\n",
    "                elif act == 'prelu':\n",
    "                    m.append(nn.PReLU(n_feats))\n",
    "\n",
    "        elif scale == 3:\n",
    "            m.append(conv(n_feats, 9 * n_feats, 3, bias))\n",
    "            m.append(nn.PixelShuffle(3))\n",
    "            if bn:\n",
    "                m.append(nn.BatchNorm2d(n_feats))\n",
    "            if act == 'relu':\n",
    "                m.append(nn.ReLU(True))\n",
    "            elif act == 'prelu':\n",
    "                m.append(nn.PReLU(n_feats))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        super(Upsampler, self).__init__(*m)\n",
    "\n",
    "\n",
    "class MeanShift(nn.Conv2d):\n",
    "    def __init__(\n",
    "            self, rgb_range,\n",
    "            rgb_mean=(0.4488, 0.4371, 0.4040), rgb_std=(1.0, 1.0, 1.0), sign=-1):\n",
    "        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n",
    "        std = torch.Tensor(rgb_std)\n",
    "        self.weight.data = torch.eye(3).view(3, 3, 1, 1) / std.view(3, 1, 1, 1)\n",
    "        self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean) / std\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "@register('edsr-baseline')\n",
    "class EDSR(nn.Module):\n",
    "    def __init__(self, args, conv=default_conv):\n",
    "        super(EDSR, self).__init__()\n",
    "        self.args = args\n",
    "        n_resblocks = args.n_resblocks\n",
    "        n_feats = args.n_feats\n",
    "        kernel_size = 3\n",
    "        scale = args.scale[0]\n",
    "        act = nn.ReLU(True)\n",
    "\n",
    "        # define head module\n",
    "        m_head = [conv(args.n_colors, n_feats, kernel_size)]\n",
    "\n",
    "        # define body module\n",
    "        m_body = [\n",
    "            ResBlock(\n",
    "                conv, n_feats, kernel_size, act=act, res_scale=args.res_scale\n",
    "            ) for _ in range(n_resblocks)\n",
    "        ]\n",
    "        m_body.append(conv(n_feats, n_feats, kernel_size))\n",
    "\n",
    "        self.head = nn.Sequential(*m_head)\n",
    "        self.body = nn.Sequential(*m_body)\n",
    "\n",
    "        if args.no_upsampling:\n",
    "            self.out_dim = n_feats\n",
    "        else:\n",
    "            self.out_dim = args.n_colors\n",
    "            # define tail module\n",
    "            m_tail = [\n",
    "                Upsampler(conv, scale, n_feats, act=False),\n",
    "                conv(n_feats, args.n_colors, kernel_size)\n",
    "            ]\n",
    "            self.tail = nn.Sequential(*m_tail)\n",
    "\n",
    "        self.sub_mean = MeanShift(args.rgb_range)\n",
    "        self.add_mean = MeanShift(args.rgb_range, sign=1)\n",
    "\n",
    "        if args.pretrained_path:\n",
    "            try:\n",
    "                pretrained_dict = torch.load(args.pretrained_path)\n",
    "                self.load_state_dict(pretrained_dict)\n",
    "                print(\"Pretrained model loaded successfully.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {args.pretrained_path}\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error loading model: {e}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.head(x)\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "\n",
    "        if self.args.no_upsampling:\n",
    "            x = res\n",
    "        else:\n",
    "            x = self.tail(res)\n",
    "        return x\n",
    "\n",
    "    def load_state_dict(self, state_dict, strict=True):\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in state_dict.items():\n",
    "            if name in own_state:\n",
    "                if isinstance(param, nn.Parameter):\n",
    "                    param = param.data\n",
    "                try:\n",
    "                    own_state[name].copy_(param)\n",
    "                except RuntimeError:\n",
    "                    if name.find('tail') == -1:\n",
    "                        raise RuntimeError('While copying the parameter named {}, '\n",
    "                                           'whose dimensions in the model are {} and '\n",
    "                                           'whose dimensions in the checkpoint are {}.'\n",
    "                                           .format(name, own_state[name].size(), param.size()))\n",
    "            elif strict:\n",
    "                if name.find('tail') == -1:\n",
    "                    raise KeyError('unexpected key \"{}\" in state_dict'\n",
    "                                   .format(name))\n",
    "\n",
    "\n",
    "class SegmentationHead(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):\n",
    "        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n",
    "        super().__init__(conv2d, upsampling)\n",
    "\n",
    "@register(\"encoder\")\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args, n_class=100):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.args = args\n",
    "        self.edsr = EDSR(args)\n",
    "        self.segmentation_head = SegmentationHead(in_channels=args.n_feats, out_channels=n_class)\n",
    "        self.out_dim = args.n_feats\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.edsr(x)\n",
    "        logits = self.segmentation_head(x)\n",
    "        B, Class, H, W = logits.shape\n",
    "        logits = logits.permute(0, 2, 3, 1).contiguous().view(B * H * W, Class)\n",
    "        if self.training:\n",
    "            logits = F.gumbel_softmax(logits, tau=1, hard=False)\n",
    "        if not self.training:\n",
    "            logits = F.gumbel_softmax(logits, tau=1, hard=True)\n",
    "        logits = logits.view(B, H, W, Class).permute(0, 3, 1, 2).contiguous()\n",
    "        return x, logits\n",
    "\n",
    "\n",
    "@register('edsr-baseline')\n",
    "def make_encoder_baseline(n_resblocks=16, n_feats=64, res_scale=1, scale=2, no_upsampling=True, rgb_range=1, n_class=100):\n",
    "    url = {\n",
    "        'r16f64x2': 'models/weights/edsr_baseline_x2-1bc95232.pt',\n",
    "        'r16f64x3': 'models/weights/edsr_baseline_x3-abf2a44e.pt',\n",
    "        'r16f64x4': 'models/weights/edsr_baseline_x4-6b446fab.pt',\n",
    "        'r32f256x2': 'models/weights/edsr_x2-0edfb8a3.pt',\n",
    "        'r32f256x3': 'models/weights/edsr_x3-ea3ef2c6.pt',\n",
    "        'r32f256x4': 'models/weights/edsr_x4-4f62e9ef.pt'\n",
    "    }\n",
    "    url_name = 'r{}f{}x{}'.format(n_resblocks, n_feats, scale)\n",
    "    if url_name in url:\n",
    "        url = url[url_name]\n",
    "    else:\n",
    "        url = None\n",
    "    args = Namespace()\n",
    "    args.n_resblocks = n_resblocks\n",
    "    args.n_feats = n_feats\n",
    "    args.res_scale = res_scale\n",
    "    args.scale = [scale]\n",
    "    args.no_upsampling = no_upsampling\n",
    "    args.rgb_range = rgb_range\n",
    "    args.n_colors = 3\n",
    "    args.pretrained_path = url\n",
    "    return Encoder(args, n_class)\n",
    "\n",
    "\n",
    "@register('edsr-large')\n",
    "def make_encoder_large(n_resblocks=32, n_feats=256, res_scale=0.1, scale=2, no_upsampling=True, rgb_range=1, n_class=100):\n",
    "    url = {\n",
    "        'r16f64x2': 'models/weights/edsr_baseline_x2-1bc95232.pt',\n",
    "        'r16f64x3': 'models/weights/edsr_baseline_x3-abf2a44e.pt',\n",
    "        'r16f64x4': 'models/weights/edsr_baseline_x4-6b446fab.pt',\n",
    "        'r32f256x2': 'models/weights/edsr_x2-0edfb8a3.pt',\n",
    "        'r32f256x3': 'models/weights/edsr_x3-ea3ef2c6.pt',\n",
    "        'r32f256x4': 'models/weights/edsr_x4-4f62e9ef.pt'\n",
    "    }\n",
    "    url_name = 'r{}f{}x{}'.format(n_resblocks, n_feats, scale)\n",
    "    if url_name in url:\n",
    "        url = url[url_name]\n",
    "    else:\n",
    "        url = None\n",
    "    args = Namespace()\n",
    "    args.n_resblocks = n_resblocks\n",
    "    args.n_feats = n_feats\n",
    "    args.res_scale = res_scale\n",
    "    args.scale = [scale]\n",
    "    args.no_upsampling = no_upsampling\n",
    "    args.rgb_range = rgb_range\n",
    "    args.n_colors = 3\n",
    "    args.pretrained_path = url\n",
    "    return Encoder(args, n_class)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    network = make_encoder_baseline()\n",
    "    a = torch.rand(1, 3, 48, 48)\n",
    "    print(network(a)[0].shape, network(a)[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: models/weights/edsr_baseline_x2-1bc95232.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/g8qdvht528xc9srh5br0f6jc0000gn/T/ipykernel_30310/3632505019.py:123: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(args.pretrained_path)\n",
      "/Users/byungwanlim/miniconda3/envs/UM2/lib/python3.12/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 380\u001b[0m\n\u001b[1;32m    378\u001b[0m cell_[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m ny\n\u001b[1;32m    379\u001b[0m sr_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhr_coord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/UM2/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/UM2/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 352\u001b[0m, in \u001b[0;36mGaussianSplatter.forward\u001b[0;34m(self, inp, coord, scale, cell)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp, coord, scale, cell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_feat\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_rgb(coord, scale, cell)\n",
      "Cell \u001b[0;32mIn[8], line 196\u001b[0m, in \u001b[0;36mGaussianSplatter.gen_feat\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minp \u001b[38;5;241m=\u001b[39m inp\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(inp)\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat_coord \u001b[38;5;241m=\u001b[39m \u001b[43mmake_coord\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \\\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(inp\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m*\u001b[39minp\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/miniconda3/envs/UM2/lib/python3.12/site-packages/torch/cuda/__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import models\n",
    "from models import register\n",
    "\n",
    "from utils import to_pixel_samples\n",
    "\n",
    "\n",
    "def default_conv(in_channels, out_channels, kernel_size, bias=True):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size // 2), bias=bias)\n",
    "\n",
    "\n",
    "def make_coord(shape, ranges=None, flatten=True):\n",
    "    \"\"\" Make coordinates at grid centers.\"\"\"\n",
    "    coord_seqs = []\n",
    "    for i, n in enumerate(shape):\n",
    "        if ranges is None:\n",
    "            v0, v1 = -1, 1\n",
    "        else:\n",
    "            v0, v1 = ranges[i]\n",
    "        r = (v1 - v0) / (2 * n)\n",
    "        seq = v0 + r + (2 * r) * torch.arange(n).float()\n",
    "        coord_seqs.append(seq)\n",
    "    ret = torch.stack(torch.meshgrid(*coord_seqs), dim=-1)\n",
    "    if flatten:\n",
    "        ret = ret.view(-1, ret.shape[-1])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def generate_meshgrid(height, width):\n",
    "    \"\"\"\n",
    "    Generate a meshgrid of coordinates for a given image dimensions.\n",
    "    Args:\n",
    "        height (int): Height of the image.\n",
    "        width (int): Width of the image.\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape [height * width, 2] containing the (x, y) coordinates for each pixel in the image.\n",
    "    \"\"\"\n",
    "    # Generate all pixel coordinates for the given image dimensions\n",
    "    y_coords, x_coords = torch.arange(0, height), torch.arange(0, width)\n",
    "    # Create a grid of coordinates\n",
    "    yy, xx = torch.meshgrid(y_coords, x_coords)\n",
    "    # Flatten and stack the coordinates to obtain a list of (x, y) pairs\n",
    "    all_coords = torch.stack([xx.flatten(), yy.flatten()], dim=1)\n",
    "    return all_coords\n",
    "\n",
    "\n",
    "def fetching_features_from_tensor(image_tensor, input_coords):\n",
    "    \"\"\"\n",
    "    Extracts pixel values from a tensor of images at specified coordinate locations.\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): A 4D tensor of shape [batch, channel, height, width] representing a batch of images.\n",
    "        input_coords (torch.Tensor): A 2D tensor of shape [N, 2] containing the (x, y) coordinates at which to extract pixel values.\n",
    "    Returns:\n",
    "        color_values (torch.Tensor): A 3D tensor of shape [batch, N, channel] containing the pixel values at the specified coordinates.\n",
    "        coords (torch.Tensor): A 2D tensor of shape [N, 2] containing the normalized coordinates in the range [-1, 1].\n",
    "    \"\"\"\n",
    "    # Normalize pixel coordinates to [-1, 1] range\n",
    "    input_coords = input_coords.to(image_tensor.device)\n",
    "    coords = input_coords / torch.tensor([image_tensor.shape[-2], image_tensor.shape[-1]],\n",
    "                                         device=image_tensor.device).float()\n",
    "    center_coords_normalized = torch.tensor([0.5, 0.5], device=image_tensor.device).float()\n",
    "    coords = (center_coords_normalized - coords) * 2.0\n",
    "\n",
    "    # Fetching the colour of the pixels in each coordinates\n",
    "    batch_size = image_tensor.shape[0]\n",
    "    input_coords_expanded = input_coords.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "    y_coords = input_coords_expanded[..., 0].long()\n",
    "    x_coords = input_coords_expanded[..., 1].long()\n",
    "    batch_indices = torch.arange(batch_size).view(-1, 1).to(input_coords.device)\n",
    "\n",
    "    color_values = image_tensor[batch_indices, :, x_coords, y_coords]\n",
    "\n",
    "    return color_values, coords\n",
    "\n",
    "\n",
    "def extract_patch(image, center, radius, padding_mode='constant'):\n",
    "    \"\"\"\n",
    "    Extract a patch from an image with the specified center and radius.\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image of shape [batch_size, channels, height, width].\n",
    "        center (tuple): Coordinates (y, x) of the patch center.\n",
    "        radius (int): Radius of the patch.\n",
    "        padding_mode (str, optional): Padding mode, can be 'constant', 'reflect', 'replicate', or 'circular'. Default is 'constant'.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Extracted patch of shape [batch_size, channels, 2 * radius, 2 * radius].\n",
    "    \"\"\"\n",
    "    height, width = image.shape[-2:]\n",
    "\n",
    "    # Convert center coordinates to integers\n",
    "    center_y, center_x = int(round(center[0])), int(round(center[1]))\n",
    "\n",
    "    # Calculate patch boundaries\n",
    "    top = center_y - radius\n",
    "    bottom = center_y + radius\n",
    "    left = center_x - radius\n",
    "    right = center_x + radius\n",
    "\n",
    "    # Check if boundaries are out of image bounds\n",
    "    top_padding = max(0, -top)\n",
    "    bottom_padding = max(0, bottom - height)\n",
    "    left_padding = max(0, -left)\n",
    "    right_padding = max(0, right - width)\n",
    "\n",
    "    # Pad the image\n",
    "    padded_image = torch.nn.functional.pad(image, (left_padding, right_padding, top_padding, bottom_padding),\n",
    "                                           mode=padding_mode)\n",
    "\n",
    "    # Extract the patch\n",
    "    patch = padded_image[..., top_padding:top_padding + 2 * radius, left_padding:left_padding + 2 * radius]\n",
    "\n",
    "    return patch\n",
    "\n",
    "\n",
    "@register('gaussian-splatter')\n",
    "class GaussianSplatter(nn.Module):\n",
    "    \"\"\"A module that applies 2D Gaussian splatting to input features.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder_spec, dec_spec, kernel_size, hidden_dim=256, unfold_row=7, unfold_column=7,\n",
    "                 num_points=100):\n",
    "        \"\"\"\n",
    "        Initialize the 2D Gaussian Splatter module.\n",
    "        Args:\n",
    "            kernel_size (int): The size of the kernel to convert rasterization.\n",
    "            unfold_row (int): The number of points in the row dimension of the Gaussian grid.\n",
    "            unfold_column (int): The number of points in the column dimension of the Gaussian grid.\n",
    "        \"\"\"\n",
    "        super(GaussianSplatter, self).__init__()\n",
    "        self.encoder = models.make(encoder_spec)\n",
    "        self.feat, self.logits = None, None\n",
    "\n",
    "        self.coef = nn.Conv2d(self.encoder.out_dim, hidden_dim, 3, padding=1)\n",
    "        self.freq = nn.Conv2d(self.encoder.out_dim, hidden_dim, 3, padding=1)\n",
    "        self.phase = nn.Linear(2, hidden_dim // 2, bias=False)\n",
    "        self.dec = models.make(dec_spec, args={'in_dim': hidden_dim})\n",
    "\n",
    "        # Key parameter in 2D Gaussian Splatter\n",
    "        self.kernel_size = kernel_size\n",
    "        self.row = unfold_row\n",
    "        self.column = unfold_column\n",
    "        self.num_points = num_points\n",
    "\n",
    "        # Initialize Trainable Parameters\n",
    "        sigma_x, sigma_y = torch.meshgrid(torch.linspace(0.2, 3.0, 10), torch.linspace(0.2, 3.0, 10))\n",
    "        self.sigma_x = sigma_x.reshape(-1)\n",
    "        self.sigma_y = sigma_y.reshape(-1)\n",
    "        self.opacity = torch.sigmoid(torch.ones(self.num_points, 1, requires_grad=True))\n",
    "        self.rho = torch.clamp(torch.zeros(self.num_points, 1, requires_grad=True), min=-1, max=1)\n",
    "        self.sigma_x = nn.Parameter(self.sigma_x)  # Standard deviation in x-axis\n",
    "        self.sigma_y = nn.Parameter(self.sigma_y)  # Standard deviation in y-axis\n",
    "        self.opacity = nn.Parameter(self.opacity)  # Transparency of feature, shape=[num_points, 1]\n",
    "        self.rho = nn.Parameter(self.rho)\n",
    "\n",
    "    def weighted_gaussian_parameters(self, logits):\n",
    "        \"\"\"\n",
    "        Computes weighted Gaussian parameters based on logits and the Gaussian kernel parameters (sigma_x, sigma_y, opacity).\n",
    "        The logits tensor is used as a weight to compute a weighted sum of the Gaussian kernel parameters for each spatial\n",
    "        location across the batch dimension. The resulting weighted parameters are then averaged across the batch dimension.\n",
    "        Args:\n",
    "            logits (torch.Tensor): Logits tensor of shape [batch, class, height, width].\n",
    "        Returns:\n",
    "            tuple: A tuple containing the weighted Gaussian parameters:\n",
    "                - weighted_sigma_x (torch.Tensor): Tensor of shape [height * width] representing the weighted x-axis standard deviations.\n",
    "                - weighted_sigma_y (torch.Tensor): Tensor of shape [height * width] representing the weighted y-axis standard deviations.\n",
    "                - weighted_opacity (torch.Tensor): Tensor of shape [height * width] representing the weighted opacities.\n",
    "        Description:\n",
    "            This function computes weighted Gaussian parameters based on the input tensor, logits, and the provided Gaussian kernel parameters (sigma_x, sigma_y, and opacity). The logits tensor is used as a weight to compute a weighted sum of the Gaussian kernel parameters for each spatial location (height and width) across the batch dimension. The resulting weighted parameters are then averaged across the batch dimension, yielding tensors of shape [height * width] for the weighted sigma_x, sigma_y, and opacity.\n",
    "        \"\"\"\n",
    "        batch_size, num_classes, height, width = logits.size()\n",
    "        logits = logits.permute(0, 2, 3, 1)  # Reshape logits to [batch, height, width, class]\n",
    "\n",
    "        # Compute weighted sum of Gaussian parameters across class dimension\n",
    "        weighted_sigma_x = (logits * self.sigma_x.unsqueeze(0).unsqueeze(0).unsqueeze(0)).sum(dim=-1)\n",
    "        weighted_sigma_y = (logits * self.sigma_y.unsqueeze(0).unsqueeze(0).unsqueeze(0)).sum(dim=-1)\n",
    "        weighted_opacity = (logits * self.opacity[:, 0].unsqueeze(0).unsqueeze(0).unsqueeze(0)).sum(dim=-1)\n",
    "        weighted_rho = (logits * self.rho[:, 0].unsqueeze(0).unsqueeze(0).unsqueeze(0)).sum(dim=-1)\n",
    "\n",
    "        # Reshape and average across batch dimension\n",
    "        weighted_sigma_x = weighted_sigma_x.reshape(batch_size, -1).mean(dim=0)\n",
    "        weighted_sigma_y = weighted_sigma_y.reshape(batch_size, -1).mean(dim=0)\n",
    "        weighted_opacity = weighted_opacity.reshape(batch_size, -1).mean(dim=0)\n",
    "        weighted_rho = weighted_rho.reshape(batch_size, -1).mean(dim=0)\n",
    "\n",
    "        return weighted_sigma_x, weighted_sigma_y, weighted_opacity, weighted_rho\n",
    "\n",
    "    def gen_feat(self, inp):\n",
    "        \"\"\"Generate feature and logits by encoder.\"\"\"\n",
    "        self.inp = inp\n",
    "        self.feat, self.logits = self.encoder(inp)\n",
    "        self.feat_coord = make_coord(inp.shape[-2:], flatten=False).cuda().permute(2, 0, 1) \\\n",
    "            .unsqueeze(0).expand(inp.shape[0], 2, *inp.shape[-2:])\n",
    "        return self.feat, self.logits\n",
    "\n",
    "    def query_rgb(self, coord, scale, cell=None):\n",
    "        \"\"\"\n",
    "        Continuous sampling through 2D Gaussian Splatting.\n",
    "        Args:\n",
    "            coord (torch.Tensor): [Batch, Sample_q, 2]. The normalized coordinates of HR space (of range [-1, 1]).\n",
    "            cell (torch.Tensor): [Batch, Sample_q, 2]. The normalized cell size of HR space.\n",
    "            scale (torch.Tensor): [Batch]. The magnification scale of super-resolution. (1, 4) during training.\n",
    "        Returns:\n",
    "            torch.Tensor: The output features after Gaussian splatting, of the same shape as the input.\n",
    "        \"\"\"\n",
    "        # 1. Get LR feature and logits\n",
    "        feat, lr_feat, logits = self.feat[:, :8, :, :], self.feat[:, 8:, :, :], self.logits  # Channel decoupling\n",
    "        feat_size, feat_device = feat.shape, feat.device\n",
    "\n",
    "        # 2. Calculate the high-resolution image size\n",
    "        scale = float(scale[0])\n",
    "        hr_h = round(feat.shape[-2] * scale)  # shape: [batch size]\n",
    "        hr_w = round(feat.shape[-1] * scale)\n",
    "\n",
    "        # 3. Unfold the feature / logits to many small patches to avoid extreme GPU memory consumption\n",
    "        num_kernels_row = math.ceil(feat_size[-2] / self.row)\n",
    "        num_kernels_column = math.ceil(feat_size[-1] / self.column)\n",
    "        upsampled_size = (num_kernels_row * self.row, num_kernels_column * self.column)\n",
    "        upsampled_inp = F.interpolate(feat, size=upsampled_size, mode='bicubic', align_corners=False)\n",
    "        upsampled_logits = F.interpolate(logits, size=upsampled_size, mode='bicubic', align_corners=False)\n",
    "        unfold = nn.Unfold(kernel_size=(self.row, self.column), stride=(self.row, self.column))\n",
    "        unfolded_feature = unfold(upsampled_inp)\n",
    "        unfolded_logits = unfold(upsampled_logits)\n",
    "        # Unfolded_feature dimension becomes [Batch, C*K*K, L], where L is the number of columns after unfolding\n",
    "        L = unfolded_feature.shape[-1]\n",
    "        unfolded_feature_reshaped = unfolded_feature.transpose(1, 2). \\\n",
    "            reshape(feat_size[0] * L, feat_size[1], self.row, self.column)\n",
    "        unfold_feat = unfolded_feature_reshaped  # shape: [num of patch * batch, channel, self.row, self.column]\n",
    "        unfolded_logits_reshaped = unfolded_logits.transpose(1, 2). \\\n",
    "            reshape(logits.shape[0] * L, logits.shape[1], self.row, self.column)\n",
    "        unfold_logits = unfolded_logits_reshaped  # shape: [num of patch * batch, channel, self.row, self.column]\n",
    "\n",
    "        # 4. Generate colors_(features) and coords_norm\n",
    "        coords_ = generate_meshgrid(unfold_feat.shape[-2], unfold_feat.shape[-1])\n",
    "        num_LR_points = unfold_feat.shape[-2] * unfold_feat.shape[-1]\n",
    "        colors_, coords_norm = fetching_features_from_tensor(unfold_feat, coords_)\n",
    "\n",
    "        # 5. Rasterization: Generating grid\n",
    "        # 5.1. Spread Gaussian points over the whole feature map\n",
    "        batch_size, channel, _, _ = unfold_feat.shape\n",
    "        weighted_sigma_x, weighted_sigma_y, weighted_opacity, weighted_rho = self.weighted_gaussian_parameters(\n",
    "            unfold_logits)\n",
    "        sigma_x = weighted_sigma_x.view(num_LR_points, 1, 1)\n",
    "        sigma_y = weighted_sigma_y.view(num_LR_points, 1, 1)\n",
    "        rho = weighted_rho.view(num_LR_points, 1, 1)\n",
    "\n",
    "        # 5.2. Gaussian expression\n",
    "        covariance = torch.stack(\n",
    "            [torch.stack([sigma_x ** 2 + 1e-5, rho * sigma_x * sigma_y], dim=-1),\n",
    "             torch.stack([rho * sigma_x * sigma_y, sigma_y ** 2 + 1e-5], dim=-1)], dim=-2\n",
    "        )  # when correlation rou is set to zero, covariance will always be positive semi-definite\n",
    "        inv_covariance = torch.inverse(covariance).to(feat_device)\n",
    "\n",
    "        # 5.3. Choosing a broad range for the distribution [-5,5] to avoid any clipping\n",
    "        start = torch.tensor([-5.0], device=feat_device).view(-1, 1)\n",
    "        end = torch.tensor([5.0], device=feat_device).view(-1, 1)\n",
    "        base_linspace = torch.linspace(0, 1, steps=self.kernel_size, device=feat_device)\n",
    "        ax_batch = start + (end - start) * base_linspace\n",
    "        # Expanding dims for broadcasting\n",
    "        ax_batch_expanded_x = ax_batch.unsqueeze(-1).expand(-1, -1, self.kernel_size)\n",
    "        ax_batch_expanded_y = ax_batch.unsqueeze(1).expand(-1, self.kernel_size, -1)\n",
    "\n",
    "        # 5.4. Creating a batch-wise meshgrid using broadcasting\n",
    "        xx, yy = ax_batch_expanded_x, ax_batch_expanded_y\n",
    "        xy = torch.stack([xx, yy], dim=-1)\n",
    "        z = torch.einsum('b...i,b...ij,b...j->b...', xy, -0.5 * inv_covariance, xy)\n",
    "        kernel = torch.exp(z) / (2 * torch.tensor(np.pi, device=feat_device) *\n",
    "                                 torch.sqrt(torch.det(covariance)).to(feat_device).view(num_LR_points, 1, 1))\n",
    "        kernel_max_1, _ = kernel.max(dim=-1, keepdim=True)  # Find max along the last dimension\n",
    "        kernel_max_2, _ = kernel_max_1.max(dim=-2, keepdim=True)  # Find max along the second-to-last dimension\n",
    "        kernel_normalized = kernel / kernel_max_2\n",
    "        kernel_reshaped = kernel_normalized.repeat(1, channel, 1).contiguous(). \\\n",
    "            view(num_LR_points * channel, self.kernel_size, self.kernel_size)\n",
    "        kernel_color = kernel_reshaped.unsqueeze(0).reshape(num_LR_points, channel, self.kernel_size, self.kernel_size)\n",
    "\n",
    "        # 5.5. Adding padding to make kernel size equal to the image size\n",
    "        pad_h = round(unfold_feat.shape[-2] * scale) - self.kernel_size\n",
    "        pad_w = round(unfold_feat.shape[-1] * scale) - self.kernel_size\n",
    "        if pad_h < 0 or pad_w < 0:\n",
    "            raise ValueError(\"Kernel size should be smaller or equal to the image size.\")\n",
    "        padding = (pad_w // 2, pad_w // 2 + pad_w % 2, pad_h // 2, pad_h // 2 + pad_h % 2)\n",
    "        kernel_color_padded = torch.nn.functional.pad(kernel_color, padding, \"constant\", 0)\n",
    "\n",
    "        # 5.6. Create a batch of 2D affine matrices\n",
    "        b, c, h, w = kernel_color_padded.shape  # num_LR_points, channel, hr_h, hr_w\n",
    "        theta = torch.zeros(batch_size, b, 2, 3, dtype=torch.float32, device=feat_device)\n",
    "        theta[:, :, 0, 0] = 1.0\n",
    "        theta[:, :, 1, 1] = 1.0\n",
    "        theta[:, :, :, 2] = coords_norm\n",
    "        grid = F.affine_grid(theta.view(-1, 2, 3), size=[batch_size * b, c, h, w], align_corners=True).contiguous()\n",
    "        kernel_color_padded_expanded = kernel_color_padded.repeat(batch_size, 1, 1, 1).contiguous()\n",
    "        kernel_color_padded_translated = F.grid_sample(kernel_color_padded_expanded.contiguous(), grid.contiguous(),\n",
    "                                                       align_corners=True)\n",
    "        kernel_color_padded_translated = kernel_color_padded_translated.view(batch_size, b, c, h, w)\n",
    "\n",
    "        # 6. Apply Gaussian splatting\n",
    "        # colors_.shape = [batch, num_LR_points, channel], colors.shape = [batch, num_LR_points, channel]\n",
    "        colors = colors_ * weighted_opacity.to(feat_device).unsqueeze(-1).expand(batch_size, -1, -1)\n",
    "        color_values_reshaped = colors.unsqueeze(-1).unsqueeze(-1)\n",
    "        final_image_layers = color_values_reshaped * kernel_color_padded_translated\n",
    "        final_image = final_image_layers.sum(dim=1)\n",
    "        final_image = torch.clamp(final_image, 0, 1)\n",
    "\n",
    "        # 7. Fold the input back to the original size\n",
    "        # Calculate the number of kernels needed to cover each dimension.\n",
    "        kernel_h, kernel_w = round(self.row * scale), round(self.column * scale)\n",
    "        fold = nn.Fold(output_size=(kernel_h * num_kernels_row, kernel_w * num_kernels_column),\n",
    "                       kernel_size=(kernel_h, kernel_w), stride=(kernel_h, kernel_w))\n",
    "        final_image = final_image.reshape(feat_size[0], L, feat_size[1] * kernel_h * kernel_w).transpose(1, 2)\n",
    "        final_image = fold(final_image)\n",
    "        final_image = F.interpolate(final_image, size=(hr_h, hr_w), mode='bicubic', align_corners=False)\n",
    "        # Combine channel\n",
    "        lr_feat = F.interpolate(lr_feat, size=(hr_h, hr_w), mode='bicubic', align_corners=False)\n",
    "        final_image = torch.concat((final_image, lr_feat), dim=1)\n",
    "\n",
    "        # 8. Augmentation (Useful for improving out-of-distribution scale performance)\n",
    "        coef = self.coef(final_image)\n",
    "        freq = self.freq(final_image)\n",
    "        feat_coord = self.feat_coord\n",
    "        coord_ = coord.clone()\n",
    "        q_coef = F.grid_sample(coef, coord_.flip(-1).unsqueeze(1), mode='nearest', align_corners=False)[:, :, 0, :] \\\n",
    "            .permute(0, 2, 1)\n",
    "        q_freq = F.grid_sample(freq, coord_.flip(-1).unsqueeze(1), mode='nearest', align_corners=False)[:, :, 0, :] \\\n",
    "            .permute(0, 2, 1)\n",
    "        q_coord = F.grid_sample(feat_coord, coord_.flip(-1).unsqueeze(1), mode='nearest', align_corners=False)[:, :, 0,\n",
    "                  :] \\\n",
    "            .permute(0, 2, 1)\n",
    "        rel_coord = coord - q_coord\n",
    "        rel_coord[:, :, 0] *= feat.shape[-2]\n",
    "        rel_coord[:, :, 1] *= feat.shape[-1]\n",
    "        rel_cell = cell.clone()\n",
    "        rel_cell[:, :, 0] *= feat.shape[-2]\n",
    "        rel_cell[:, :, 1] *= feat.shape[-1]\n",
    "        bs, q = coord.shape[:2]\n",
    "        q_freq = torch.stack(torch.split(q_freq, 2, dim=-1), dim=-1)\n",
    "        q_freq = torch.mul(q_freq, rel_coord.unsqueeze(-1))\n",
    "        q_freq = torch.sum(q_freq, dim=-2)\n",
    "        q_freq += self.phase(rel_cell.view((bs * q, -1))).view(bs, q, -1)\n",
    "        q_freq = torch.cat((torch.cos(np.pi * q_freq), torch.sin(np.pi * q_freq)), dim=-1)\n",
    "\n",
    "        inp = torch.mul(q_coef, q_freq)\n",
    "\n",
    "        pred = self.dec(inp.contiguous().view(bs * q, -1)).view(bs, q, -1)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def forward(self, inp, coord, scale, cell=None):\n",
    "        self.gen_feat(inp)\n",
    "        return self.query_rgb(coord, scale, cell)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # A simple example of implementing class GaussianSplatter\n",
    "    model = GaussianSplatter(encoder_spec={\"name\": \"edsr-baseline\", \"args\": {\"no_upsampling\": True}},\n",
    "                             dec_spec={\"name\": \"mlp\", \"args\": {\"out_dim\": 3, \"hidden_list\": [256, 256, 256, 256]}},\n",
    "                             kernel_size=3)\n",
    "    input = torch.rand(1, 3, 64, 64)\n",
    "    sr_scale = 2\n",
    "    hr_coord, hr_rgb = to_pixel_samples(\n",
    "        F.interpolate(input, size=(round(input.shape[-2] * sr_scale), round(input.shape[-1] * sr_scale)),\n",
    "                      mode='bicubic', align_corners=False))\n",
    "    v0_x, v1_x, v0_y, v1_y = -1, 1, -1, 1\n",
    "    nx, ny = round(input.shape[-2] * sr_scale), round(input.shape[-1] * sr_scale)\n",
    "\n",
    "    x = ((hr_coord[..., 0] - v0_x) / (v1_x - v0_x) * 2 * (nx - 1) / 2).round().long()\n",
    "    y = ((hr_coord[..., 1] - v0_y) / (v1_y - v0_y) * 2 * (ny - 1) / 2).round().long()\n",
    "    restored_coords = torch.stack([x, y], dim=-1)\n",
    "\n",
    "    sample_lst = np.random.choice(len(hr_coord), 2304, replace=False)\n",
    "    hr_coord = hr_coord[sample_lst]\n",
    "    hr_rgb = hr_rgb[sample_lst]\n",
    "    cell_ = torch.ones_like(hr_coord.unsqueeze(0))\n",
    "    cell_[:, 0] *= 2 / nx\n",
    "    cell_[:, 1] *= 2 / ny\n",
    "    sr_scale = 2 * torch.ones(1)\n",
    "    print(model(input, hr_coord.unsqueeze(0), sr_scale, cell_).shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UM2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
